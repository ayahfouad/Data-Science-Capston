## Milestone Project for Data Science Capstone

### Report purpose

The goal of this project is just to display that you’ve gotten used to working with the data and that you are on track to create your prediction algorithm. Please submit a report on R Pubs (http://rpubs.com/) that explains your exploratory analysis and your goals for the eventual app and algorithm. This document should be concise and explain only the major features of the data you have identified and briefly summarize your plans for creating the prediction algorithm and Shiny app in a way that would be understandable to a non-data scientist manager. You should make use of tables and plots to illustrate important summaries of the data set.

The motivation for this project is to:

1- Demonstrate that you’ve downloaded the data and have successfully loaded it in.  
2- Create a basic report of summary statistics about the data sets.  
3- Report any interesting findings that you amassed so far.    
4- Get feedback on your plans for creating a prediction algorithm and Shiny app.


```{r setup, include=TRUE, echo=TRUE}
knitr::opts_chunk$set(echo = TRUE)
```

### Load Data & Sample

Data set Downloading and Loading into R
Download the data from following link and unzip the files in the current working directory - https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip

```{r , cache=TRUE}
setwd("E:/Documents/new_repo/Data-Science-Capston/final/en_US")

    con <- file("en_US.news.txt", open="r")
    En_US_NEWS_text <- readLines(con); close(con)
    
    con <- file("en_US.blogs.txt", open="r")
    En_US_blogs_text <- readLines(con); close(con) 

    con <- file("en_US.twitter.txt", open="r")
    En_Twit_text <- readLines(con); close(con)
```

Extracting the following text files summary

- en_US.news.txt
- en_US.blogs.txt
- en_US.twitter.txt

```{r , cache=TRUE}
file_stat<- function(text_file, lines) {
    f_size <- file.info(text_file)[1]/1024^2
    nchars <- lapply(lines, nchar)
    maxchars <- which.max(nchars)
    word_count <- sum(sapply(strsplit(lines, "\\s+"), length))
    return(c(text_file, format(round(as.double(f_size), 2), nsmall=2), length(lines),maxchars, word_count))
}

    En_US_news_stat<- file_stat("en_US.news.txt", En_US_NEWS_text)
    En_US_blogs_stat <- file_stat("en_US.blogs.txt", En_US_blogs_text)
    En_Twit_text_stat<- file_stat("en_US.twitter.txt", En_Twit_text)

    test_summary <- c(En_US_news_stat, En_US_blogs_stat,En_Twit_text_stat)

    df <- data.frame(matrix(unlist(test_summary), nrow=3, byrow=T))
    colnames(df) <- c("Text_file", "Size(MB)", "Line_Count", "Max Line Length", "Words_Count")
    print(df)

```

### Exploratory data analysis
Here I am writing a functions to make the test data Corpus, Clean the corpus, and capture the hight frquency words
```{r , cache=TRUE}
make_Corpus<- function(test_file) {
    gen_corp<- paste(test_file, collapse=" ")
    gen_corp <- VectorSource(gen_corp)
    gen_corp <- Corpus(gen_corp)
}
    
clean_corp <- function(corp_data) {

    corp_data <- tm_map(corp_data, removeNumbers)
    corp_data <- tm_map(corp_data, content_transformer(tolower))
    corp_data <- tm_map(corp_data, removeWords, stopwords("english"))
    corp_data <- tm_map(corp_data, removePunctuation)
    corp_data <- tm_map(corp_data, stripWhitespace)
    return (corp_data)
}

high_freq_words <- function (corp_data) {
    term_sparse <- DocumentTermMatrix(corp_data)
    term_matrix <- as.matrix(term_sparse)   ## convert our term-document-matrix into a normal matrix
    freq_words <- colSums(term_matrix)
    freq_words <- as.data.frame(sort(freq_words, decreasing=TRUE))
    freq_words$word <- rownames(freq_words)
    colnames(freq_words) <- c("Frequency","word")
    return (freq_words)
}

```

### Bar Chart of High frequency words
This section is explore the different text mining commads and extract the high frequency words
```{r , cache=TRUE}
## en_US.news.txt High frequency words 
    En_US_NEWS_text1<-sample(En_US_NEWS_text, round(0.1*length(En_US_NEWS_text)), replace = F)
    US_news_corpus <- make_Corpus(En_US_NEWS_text1)
    US_news_corpus <- clean_corp(US_news_corpus)
    US_news_most_used_word <- high_freq_words(US_news_corpus)
    US_news_most_used_word1<- US_news_most_used_word[1:15,]

    p<-ggplot(data=US_news_most_used_word1, aes(x=reorder(word,Frequency), y=Frequency,
                    fill=factor(reorder(word,-Frequency))))+ geom_bar(stat="identity") 
    p + xlab("Word") +labs(title = "Most Frequent words : US News") +theme(legend.title=element_blank()) + coord_flip()
    
    
## en_US.blogs.txt High frequency words 
    En_US_blogs_text1<-sample(En_US_blogs_text, round(0.1*length(En_US_blogs_text)), replace = F)
    US_blogs_corpus <- make_Corpus(En_US_blogs_text1)
    US_blogs_corpus <- clean_corp(US_blogs_corpus)
    US_blogs_most_used_word <- high_freq_words(US_blogs_corpus)
    US_blogs_most_used_word1<- US_blogs_most_used_word[1:15,]

    p<-ggplot(data=US_blogs_most_used_word1, aes(x=reorder(word,Frequency), y=Frequency,
                    fill=factor(reorder(word,-Frequency))))+ geom_bar(stat="identity") 
    p + xlab("Word") +labs(title = "Most Frequent words : US blogs") +theme(legend.title=element_blank()) + coord_flip()
    
    
    
## en_US.twitter.txt High frequency words 
    En_Twit_text1<-sample(En_Twit_text, round(0.1*length(En_Twit_text)), replace = F)
    twitter_corpus <- make_Corpus(En_Twit_text1)
    twitter_corpus <- clean_corp(twitter_corpus)
    twitter_most_used_word <- high_freq_words(twitter_corpus)
    twitter_most_used_word1<- twitter_most_used_word[1:15,]
    
    p<-ggplot(data=twitter_most_used_word1, aes(x=reorder(word,Frequency), y=Frequency,
                    fill=factor(reorder(word,-Frequency))))+ geom_bar(stat="identity") 
    p + xlab("Word") +labs(title = "Most Frequent words : Twitter") +theme(legend.title=element_blank()) + coord_flip()
    
    

```
Generating the Word Cloud
Word Cloud is Cool representation of the Word display based on the Frequencies.
```{r , cache=TRUE}
## US News Word Cloud
library(wordcloud)
library(tokenizers)
library(stopwords)
library(quanteda)
(library(plyr))
(library(dplyr))
(library(stringi))
(library(igraph))
(library(NLP))
(library(tm))
(library(xtable))
(library(knitr))
(library(SnowballC))
(library(RWeka))
(library(ggplot2))
(library(grid))
(library(wordcloud))
(library(RColorBrewer))
(library(doParallel))
(library(slam))
(library(rvest))
(library(pipeR))
    wordcloud(US_news_most_used_word$word[1:100], US_news_most_used_word$Frequency[1:100],
              colors=brewer.pal(8, "Dark2"))

## US News Word Cloud
    wordcloud(US_blogs_most_used_word$word[1:100], US_blogs_most_used_word$Frequency[1:100],
              colors=brewer.pal(8, "Dark2")) 
    
## US Twitter Word Cloud
wordcloud(twitter_most_used_word$word[1:100], twitter_most_used_word$Frequency[1:100],
              colors=brewer.pal(8, "Dark2"))

```

Word Analysis
For the Data analysis of text document we need to create a bag of word matrices with Unigram, Bigram, Trigrams. These Ngram model set improve the predictabily of the data analysis.
```{r , cache=TRUE}
## en_US.news.txt High frequency words    
    En_US_NEWS_text1<-sample(En_US_NEWS_text, round(0.01*length(En_US_NEWS_text)), replace = F)
    US_News_tokens<- tokens(En_US_NEWS_text1,what ="word", remove_numbers = TRUE, 
                            remove_punct = TRUE, remove_separators = TRUE, remove_symbols =TRUE )
    US_News_tokens <- tokens_tolower(US_News_tokens)
    US_News_tokens <- tokens_select(US_News_tokens, stopwords(),selection ="remove")

    US_News_unigram <- tokens_ngrams(US_News_tokens, n=1)  ## unigram
    US_News_unigram.dfm <- dfm(US_News_unigram, tolower =TRUE, remove = stopwords("english"), 
                              remove_punct = TRUE)    

    US_News_bigram <- tokens_ngrams(US_News_tokens, n=2)  ## bigram
    US_News_bigram.dfm <- dfm(US_News_bigram, tolower =TRUE, remove = stopwords("english"), 
                              remove_punct = TRUE)
    
    US_News_trigram <- tokens_ngrams(US_News_tokens, n=3)  ## trigram
    US_News_trigram.dfm <- dfm(US_News_trigram, tolower =TRUE, remove = stopwords("english"), 
                              remove_punct = TRUE)
    topfeatures(US_News_unigram.dfm, 20)  # 20 top US News Unigram words
    
        topfeatures(US_News_bigram.dfm, 20)  # 20 top US News Bigram words
        
           topfeatures(US_News_trigram.dfm, 20)  # 20 top US News Trigram words
           
           
           ## en_US.blog.txt High frequency words
    En_US_blogs_text1<-sample(En_US_blogs_text, round(0.02*length(En_US_blogs_text)), replace = F)
    US_blogs_tokens<- tokens(En_US_blogs_text1,what ="word", remove_numbers = TRUE, 
                            remove_punct = TRUE, remove_separators = TRUE, remove_symbols =TRUE )
    US_blogs_tokens <- tokens_tolower(US_blogs_tokens)
    US_blogs_tokens <- tokens_select(US_blogs_tokens, stopwords(),selection ="remove")

    US_blogs_unigram <- tokens_ngrams(US_blogs_tokens, n=1)  ## unigram
    US_blogs_unigram.dfm <- dfm(US_blogs_unigram, tolower =TRUE, remove = stopwords("english"), 
                              remove_punct = TRUE)    

    US_blogs_bigram <- tokens_ngrams(US_blogs_tokens, n=2)  ## bigram
    US_blogs_bigram.dfm <- dfm(US_blogs_bigram, tolower =TRUE, remove = stopwords("english"), 
                              remove_punct = TRUE)
    
    US_blogs_trigram <- tokens_ngrams(US_blogs_tokens, n=3)  ## tiigram
    US_blogs_trigram.dfm <- dfm(US_blogs_trigram, tolower =TRUE, remove = stopwords("english"), 
                              remove_punct = TRUE)
    topfeatures(US_blogs_unigram.dfm, 20)  # 20 top US blogs Unigram words
    
     topfeatures(US_blogs_bigram.dfm, 20)  # 20 top US blogs Bigram words
     
      topfeatures(US_blogs_trigram.dfm, 20)  # 20 top US blogs Trigram words
      
      
      
      ## en_US.twitter.txt Ngram words 
    En_Twit_text1<-sample(En_Twit_text, round(0.02*length(En_Twit_text)), replace = F)
    twitter_tokens<- tokens(En_Twit_text1,what ="word", remove_numbers = TRUE, 
                            remove_punct = TRUE, remove_separators = TRUE, remove_symbols =TRUE )
    twitter_tokens <- tokens_tolower(twitter_tokens)
    twitter_tokens <- tokens_select(twitter_tokens, stopwords(),selection ="remove")

    twitter_unigram <- tokens_ngrams(twitter_tokens, n=1)  ## unigram
    twitter_unigram.dfm <- dfm(twitter_unigram, tolower =TRUE, remove = stopwords("english"), 
                              remove_punct = TRUE)    

    twitter_bigram <- tokens_ngrams(twitter_tokens, n=2)  ## bigram
    twitter_bigram.dfm <- dfm(twitter_bigram, tolower =TRUE, remove = stopwords("english"), 
                              remove_punct = TRUE)
    
    twitter_trigram <- tokens_ngrams(twitter_tokens, n=3)  ## trigram
    twitter_trigram.dfm <- dfm(twitter_trigram, tolower =TRUE, remove = stopwords("english"), 
                              remove_punct = TRUE)
    topfeatures(twitter_unigram.dfm, 20)  # 20 top Unigram words
    
      topfeatures(twitter_bigram.dfm, 20)  # 20 top Bigram words
      
      topfeatures(twitter_trigram.dfm, 20)  # 20 top  Trigram words
      
      


```

Interesting findings that you amassed so far
I have gone through the multiple literatures and youtube vidios on Text mining and “quanteda” library With Small text data sets problems how the data set will get exploded with different ngrams and Bag of words. I found quanteda library is very useful in generating the text analytics. Which very fast compare to TM library. This project motivated me to work on small samples sets to establish the my code.

Get feedback on your plans for creating a prediction algorithm and Shiny app.
Plan of Approach:
- Tockenization and bag of words with multiple Ngrams. 
- Since We need build the shiny app which have limitation on resources. I will use the small sample (~ 1        to 5%).
- I will explore the options for data compression. Run the Machine Learning programs to develop the         predictive model.
- Explore the options to improve the accuracy and speed of execution.
Feedback:
- Highly appreciate your valuable feedback on this approach and your recommendations.


